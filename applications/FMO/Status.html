<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html">
<HTML>
<BODY>
<H1> GridFMO - Progress Report </H1>

<P ALIGN="RIGHT"> last updated: Aug. 14, 2006 </P>

<H2> Progress chart </H2>

<table>
<tr><th align="left"> To do				</th><th align="left"> Status	</th></tr>
<tr><td> (1) <A HREF="#1"> Modeling		  </A>	</td><td> Done			</td></tr>
<tr><td> (2) <A HREF="#2"> Setup binaries	  </A>	</td><td> Done			</td></tr>
<tr><td> (3) <A HREF="#3"> Deployment & testing	  </A>	</td><td> Done			</td></tr>
<tr><td> (4) <A HREF="#4"> Setup Grid environment </A>	</td><td> in operation		</td></tr>
<tr><td> (5) <A HREF="#5"> Real calculation	  </A>	</td><td> in operation		</td></tr>
</table>

<P>
The second stage experiments started.

To improve the approximation, the size of each job is increased while
the number of jobs is reduced.

Devices to take load balance will become necessary, I guess.
</P>



<HR><!-- ======================================================================== -->
<H2> (1) <A NAME="1"> Modeling </A></H2>
<P>

Spacial geometries of the target molecular system were generated by
using Molecular Mechanics (MM) method.

Rough geometry is generated by algorithm, followed by the energy
minimization in MM method.

Bugs (both in my brain and in the MM program) were now removed,
hopefully, and the first series of geometries is now generated.

We will generate three more series, if the first set works.

</P>


<HR><!-- ======================================================================== -->
<H2> (2) <A NAME="2"> Setup binaries </A></H2>
<P>

(Non-grid) binaries are setting up by using ifort 9.0 compiler.

Intel libraries are linked statically (-i-static), while standard
libraries are linked dynamically.

</P>
<P>

The nightmare of the "NaN" happened with the new compiler, which is
resolved.

(It was due to the inconsistency of the floating-point precision
between intel internal form and IEEE one.)

</P>


<HR><!-- ======================================================================== -->
<H2> (3) <A NAME="3"> Deployment & testing </A></H2>

<!-- ------------------------------------------------------------ -->
<H3> Sitewise status </H3>

<table border=1>
<tr><th> Site </th><th> Install	</th><th> Test 1 (small) </th><th> Test 2 (FMO) </th></tr>
<tr><td> SDSC		</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
<tr><td> AIST		</td><td> Done	</td><td> pass	</td><td> pass 	</td></tr>
<tr><td> NCSA		</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
<tr><td> ASCC		</td><td> Done	</td><td> pass	</td><td> pass 	</td></tr>
<tr><td> Osaka-cafe	</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
<tr><td> Osaka-tea	</td><td> Done	</td><td> pass 	</td><td> pass	</td></tr>
<tr><td> JLU		</td><td> Done	</td><td> pass 	</td><td> pass	</td></tr>
<tr><td> MIMOS		</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
<tr><td> UNIZH		</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
<tr><td> BII		</td><td> Done	</td><td> pass	</td><td> pass	</td></tr>
</table>

<UL>
<LI><P>

The performance of machines is measured by Test 2. 

The results will be used for the load balancing among cluster machines.

</P>
<LI><P>

On some queue systems, it seems that parallel job submissions sink
down deeply to the bottom of the queue, when a sequence of single jobs
are submitted continuously.

Starving scheduling is necessary for the salvation, where jobs are
processed strictly FIFO manner (as in the NCSA site).

Note that, with the starving scheduling, operating rate of the
computational resources will be suffered while waiting for the
resources for the parallel job become available.

On SGE, such a starving scheduling is possible by setting
max_reservation parameter of the scheduler and by adding "-R y" option
on qsub.

</P>
<LI><P>

Old DDI library seems not to work sane with MAXCPUS = 1.

</P>
</UL>

<P>

<A HREF="Port.html">Here</A> is a short note about portability of
binaries.

</P>


<!-- ------------------------------------------------------------ -->
<H3> Configuration and performance </H3>

<table border=1>
<tr><th> site </th><th> CPU </th><th> Mem/GB </th><th> test2/min </th>
    <th> rsh? </th><th> HT </th><th> Queue </th><th> Scratch </th></tr>

<tr><td>  SDSC	</td><td> Xeon(2.4)x2 x 15	</td><td> 2	</td><td> 39,.6	</td>
    <td> NG </td><td> ON  </td><td> SGE </td><td> /state/partition1/scratch </td></tr>
<tr><td>  NCSA	</td><td> Xeon(2.0)x2 x 12	</td><td> 2	</td><td> 41.2	</td>
    <td> NG </td><td> ON  </td><td> PBS </td><td> /tmp </td></tr>
<tr><td>  ASCC	</td><td> Xeon(2.4)x1 x 3	</td><td> 1 (5)	</td><td> 151.0 (1) </td>
    <td> OK </td><td> OFF </td><td> PBS </td><td> /tmp </td></tr>
<tr><td>  AIST	</td><td> Pen3(1.4)x2 x 14	</td><td> 1	</td><td> 35.9	</td>
    <td> OK </td><td> -   </td><td> SGE </td><td> /work/ikegami </td></tr>
<tr><td>  cafe	</td><td> Xeon(2.8)x2 x 19	</td><td> 2	</td><td> 22.9	</td>
    <td> OK </td><td> ON  </td><td> SGE </td><td> /work </td></tr>
<tr><td>  tea	</td><td> Pen3(1.4)x2 x 39	</td><td> 1	</td><td> 39.1	</td>
    <td> OK </td><td> -  </td><td> SGE </td><td> /work </td></tr>
<tr><td>  JLU	</td><td> Xeon(3.0)x1 x 1	</td><td> 1	</td><td> 214.7 (3) </td>
    <td> - </td><td> ON  </td><td> fork </td><td> /tmp </td></tr>
<tr><td>  MIMOS	</td><td> Athron(0.8)x1 x 4	</td><td> 2	</td><td> 43.4 (2) </td>
    <td> NG </td><td> -  </td><td> SGE </td><td> /state/partition1 </td></tr>
<tr><td>  UNIZH	</td><td> Xeon(2.8)x2 x (6-1)	</td><td> 2	</td><td> 31.3 </td>
    <td> NG </td><td> OFF  </td><td> SGE </td><td> /state/partition1 </td></tr>
<tr><td>  BII	</td><td> Pen3(1.4)x2 x 3	</td><td> 2	</td><td> 63.5 (4) </td>
    <td> NG </td><td> -   </td><td> SGE  </td><td> /state/partition1  </td></tr>
<tr><td></td><td></td><td></td><td></td><td></td></td><td></td><td></td><td></td></tr>
<tr><td>  f32	</td><td> Xeon(3.1)x2		</td><td> 4	</td><td> 20.9	</td>
    <td> NG </td><td> OFF </td><td> SGE </td><td> /work </td></tr>
<tr><td>  p32	</td><td> Opteron(2.0)x2	</td><td> 6	</td><td> 17.8	</td>
    <td> NG </td><td> -   </td><td> PBS </td><td> /work </td></tr>
<tr><td>  m64	</td><td> Itanium(1.3)x4	</td><td> 16	</td><td> 30.6 </td>
    <td> NG </td><td> -   </td><td> PBS </td><td> /work </td></tr>
</table>

<P>
The wall clock time of test 2 is measured at node x cpu = 5x2
configuration, except that:
<OL>
<LI> On ASCC site, 2x1 configuration was used.
<LI> On MIMOS site, 4x1 configuration was used.
<LI> On JLU site, 1x1 configuration was used.
<LI> On BII site, 3x2 configuration was used.
<LI> On ASCC, pragma002 has 0.5 GB. (2006-08-30)
</OL>
</P>

<P>
Binaries are compiled by ifort 9 (minor version may vary) with option
'-O3'.  

On ume, option '-mtune pentiumpro' is added.  

For Opteron machine (p32), PathScale compiler was used.
</P>

<P>
Scores of SDSC and NCSA were not as good as expected.

The reason is unknown.

I got parallel results in 10x1 configuration, so that the
HyperThreading will not be guilty.
</P>


<HR><!-- ======================================================================== -->
<H2> (4) <A NAME="4"> Setup Grid environment	</A></H2>

<table border=1>
<tr><th> site </th><th> setup </th><th> test </th></tr>

<tr><td>  SDSC	</td><td> done </td><td> done	</td></tr>
<tr><td>  NCSA	</td><td> done </td><td> done	</td></tr>
<tr><td>  ASCC	</td><td> done </td><td> done	</td></tr>
<tr><td>  AIST	</td><td> done </td><td> done	</td></tr>
<tr><td>  cafe	</td><td> done </td><td> done	</td></tr>
<tr><td>  tea	</td><td> done </td><td> done	</td></tr>
<tr><td>  JLU	</td><td>      </td><td> 	</td></tr>
<tr><td>  MIMOS	</td><td> done </td><td> done 	</td></tr>
<tr><td>  UNIZH	</td><td>      </td><td> 	</td></tr>
<tr><td>  BII	</td><td>      </td><td> 	</td></tr>
<tr><td></td><td></td><td></td></tr>
<tr><td>  f32	</td><td> done </td><td> done	</td></tr>
<tr><td>  p32	</td><td> done </td><td> done	</td></tr>
<tr><td>  m64	</td><td> done </td><td> done	</td></tr>
</table>

<PRE>
 SDSC: globus-job-run rocks-52.sdsc.edu -env SGE_PE=mpi_2 -np 2 /bin/hostname
 NCSA: globus-job-run tgc.trecc.org/jobmanager-pbs -queue pragma /bin/hostname
 ASCC: globus-job-run pragma001.grid.sinica.edu.tw/jobmanager-pbs -queue long  /bin/hostname
 AIST: globus-job-run ume.hpcc.jp -env SGE_PE=mpich_2 -np 2 /bin/hostname
MIMOS: globus-job-run nucleus.mygridusbio.net.my -np 2 /bin/hostname
 cafe: globus-job-run cafe01.exp-net.osaka-u.ac.jp/jobmanager-sge -env SGE_PE=mpi_2 -np 2 /bin/hostname
  tea: globus-job-run tea01.exp-net.osaka-u.ac.jp/jobmanager-sge -env SGE_PE=mpi_2 -np 2 /bin/hostname

</PRE>

<UL>
<LI><P>

On NCSA, ASCC, and AIST sites, the PATH environment variable
is overridden by Globus.

As a result, paths added in .bashrc, such as $HOME/bin, are not
accessible.

It is resolved by specifying the PATH variable explicitly, like '-env
PATH=...'

<strike>
On NCSA site, however, the PATH variable seems to be immutable.

As a last resort, I have added a quick hack to the NinfG stub program
itself to modify the PATH.
</strike>

PATH overridden patch is now removed on NCSA site.

Thank you, Tom!

</P>
<LI><P>

The AIST CRL (certificate revocation list) file seems not to be
updated on <s>most</s> some of the CRL aware sites <s>(er, all of them, actually)</s>.

This causes refusal of the Globus access with the AIST certificates,
so that I cannot test at all.

Please update the AIST CRL file.

The latest AIST CRL file will be found
<A HREF="https://www.apgrid.org/CA/AIST/Production/a317c467.r0"> here</A>.

Unfortunately, the CRL file is updated on every 30-alpha days.

We are working on the automation of the update process.

</P>
<LI><P>

Different from GT3.x, the reverse lookup of server IP address is
requisite to pass the GT4.x authentication.

Sites without reverse-lookup entry cannot be accessed with GT4.0.

</P>
<LI><P>

On ASCC site, jobmanager-pbs currently uses rsh to deploy multiple-type jobs.

Unfortunately, rsh to the local host (with short name) fails on the ASCC:

<PRE>
	[pragma001]% rsh pragma001 echo
	Permission denied.
	[pragma001]% rsh pragma001.grid.sinica.edu.tw echo
	
	[pragma001]%
</PRE>

As a workaround, you have to set ~/.rhosts file.

</P>
</UL>


<HR><!-- ======================================================================== -->
<H2> (5) <A NAME="5"> Real calculation </A></H2>

<!-- ------------------------------------------------------------ -->
<H3> Algorithm and configuration </H3>

<P ALIGN="CENTER"><IMG SRC="img/GridFMO.png" WIDTH=400 HEIGHT=500></P>

<P>

In the FMO method, the electronic state of the whole molecule is
calculated by splitting it into small fragments.

The electronic state of each fragment is calculated under the
electrostatic environment posed by the other fragments.

The environment is then reconstructed from the calculated electronic
states, so that the calculation should be iterated to the
self-consistency.

To improve the accuracy, the electronic states of the fragment pairs
are also calculated after the convergence.

</P>
<P>

In the Grid implementation of FMO, there are three main players:
Bookkeeper, Doorkeeper, and Client.

The Bookkeeper keeps track of available CPU resources, while the
Doorkeeper handles actual CPU resources.

First, the Client (= the GridFMO application) asks the Bookkeeper for
available CPU resources.

Once CPUs are assigned, the Client asks the corresponding Doorkeeper
to execute a job.

Files necessary to process the job are transfered as a tar-ball to the
assigned master CPU via the Doorkeeper.

A list of assigned CPUs is also transfered, so that the job can be
processed in (fine-grained) parallel.

Resultant files after the job are transfered back, as a tar-ball,
to the Client.

Finally, the Client returns the assigned CPUs to the Bookkeper for the
recycle.

These players can be run as separate processes, or as independent
threads in a single process.

</P>
<P>

The Doorkeeper also communicates directly with the Bookkeeper; when
CPUs become available (through backend queue systems, for example),
the Doorkeeper registers them to the Bookkeeper.

When the Client detects an error in dispatching job to a cluster, the
cluster is marked as bad on the Bookkeeper.

The cluster is disabled if the number of bad marks exceeds a limit.

It may be re-enabled manually, after the trouble is removed.

</P>

<!-- ------------------------------------------------------------ -->
<H3> The first run </H3>

<P>

The first run started on 2006-05-15 18:13:32 JST and finished on
2006-05-21 06:26:03.

In this run, a single NinfG Doorkeeper was setup, which manages 7
cluster machines: SDSC, NCSA, ASCC, AIST, and three AIST Super Cluster
machine.

All Bookkeeper, Doorkeeper, and Client are launched at AIST site as
separate processes.

During the first run, Client was restarted 3 times, while Bookkeeper
and Doorkeeper were done 2 times.

</P>
<P>

Main part of Bookkeeper and Doorkeeper are written in Perl.

Thanks to the script nature, we could apply patches on those processes
on-the-fly through the interactive console attached to them.

Of cource, this console access also offeres a beautifully easy way to
blow everything up in the air.

</P>

<P ALIGN="CENTER">
  <A HREF="img/TimeLine01.png"><IMG SRC="img/TimeLine01.png" WIDTH=500 HEIGHT=260></A>
</P>

<P>

In the figure above, a part of timeline of the run is shown.

Each tiny box corresponds to an electronic state calculation of a fragment,
which was performed on the assigned nodes.

NinfG stubs are enqueued via GRAM, which are used as soon as the
machine resources become available.

When the stubs are killed (due to wall clock time limit, for example),
the assigned job is returned with error, and then is reassigned to
other machines.

</P>


<BR>
<IMG SRC="img/owl_line.gif" ALT="----------"> <BR>
<ADDRESS> Grid Technology Research Center / AIST / IKEGAMI, Tsutomu </ADDRESS> <BR>
<A HREF="FMO.html"> Back </A><BR>

</BODY>
</HTML>
